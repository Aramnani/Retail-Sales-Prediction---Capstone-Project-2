{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aramnani/Retail-Sales-Prediction---Capstone-Project-2/blob/main/Retail_Sales_Prediction_Regression_Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name**            - Aakash Ramnani"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied. My work includes various plots and graphs , visualizations , feature engineering , ensemble techniques , different ML algorithms with their respective parameter tuning , analysis and trends . Predictions are of 6 weeks of daily sales for 1,115 stores located across Germany.\n",
        "\n",
        "The goal is to predict the Sales of a given store on a given day.\n",
        "\n",
        "The main steps of the project are:\n",
        "\n",
        "**1. Basic EDA(Exploratory Data Analysis):** I have performed basic EDA to understand the data and its characteristics.\n",
        "also, have used Univariate - BI variate - and Multivarite analysis to Understanding the correlation between variables and to explore distribution and intercation of variables.\n",
        "\n",
        "**2. Hypothesis Testing:** I have performed hypothesis testing to test the relationship between the variables. Also have used statistical tests such as t-test, ANOVA, chi-square test. to compare the means or proportions of different groups or categories.\n",
        "\n",
        "**3. Feature Engineering and Data Preprocessing:** To create and select the most relevant and informative features for the model. Have used methods such as correlation analysis and VIF(Variance Inflation Factor) for feature selection.\n",
        "\n",
        "**4. ML Model Development and Evaluation:** In this project Have developed and evaluated different machine learning Models. Have used both Linear and Polynomial Regression. Also have used Tree Based Algorithm.\n",
        "\n",
        "**5. Model Interpretation and Explanation :** Explained the model predictions using feature importance. It helps to analysis the effect of different factors on the sales."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.**\n",
        "\n",
        "\n",
        "**You are provided with historical sales data for 1,115 Rossmann stores. The task is to forecast the \"Sales\" column for the test set. Note that some stores in the dataset were temporarily closed for refurbishment.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib.pylab as pylab\n",
        "import missingno as msno\n",
        "\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import f\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score,mean_squared_error\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from sklearn.linear_model import Lasso,Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!pip install shap\n",
        "import shap"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VBiouTP7MkOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df_sales = pd.read_csv('/content/drive/MyDrive/Almabetter/machine learning/project/Regression/Rossmann Stores Data.csv')\n",
        "df_stores = pd.read_csv('/content/drive/MyDrive/Almabetter/machine learning/project/Regression/store.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "# Rossman Sale Data First Look\n",
        "df_sales.head()\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rossman Stores Data First Look\n",
        "df_stores.head()"
      ],
      "metadata": {
        "id": "XYgwgP1GjkWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(df_sales.shape)\n",
        "print(df_stores.shape)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "\n",
        "# Rossman sales Data Info\n",
        "df_sales.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rossman store Data Info\n",
        "df_stores.info()"
      ],
      "metadata": {
        "id": "0zg5w1vfkEOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "# Duplicate value counts for Rossman sales Data\n",
        "len(df_sales[df_sales.duplicated])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Duplicate value counts for Rossman store Data\n",
        "len(df_stores[df_stores.duplicated])"
      ],
      "metadata": {
        "id": "bqacwEjxklBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "\n",
        "# Checking missing values in Rossman sales Dataset\n",
        "df_sales.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There are no missing values in Rossman sales Dataset."
      ],
      "metadata": {
        "id": "DWYCfWaHlCEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking missing values for Rossman store Dataset\n",
        "df_stores.isnull().sum()"
      ],
      "metadata": {
        "id": "_ZcO5m0olHsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There are missing values in CompetitionDistance,CompetitionOpenSinceMonth and CompetitionOpenSinceYear columns.\n",
        "\n",
        "- There are missing values in Promo2SinceWeek,Promo2SinceYear and PromoInterval columns."
      ],
      "metadata": {
        "id": "dF6ifbp7lVA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "# There are no missing values in Rossman sales Dataset\n",
        "# Visualizing missing values for Rossman store Dataset\n",
        "\n",
        "sns.heatmap(df_stores.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We have 2 datasets.\n",
        "  \n",
        "  1 - Rossmann Stores Data.csv - historical data including Sales\n",
        "\n",
        "  2 - store.csv - supplemental information about the stores\n",
        "\n",
        "- Rossman Sales Dataset\n",
        "\n",
        "  - Number of observation - 1017209 rows\n",
        "  - Number of columns - 9 columns\n",
        "  - Number Missing Values - 0\n",
        "  - Number of Duplicate Values - 0\n",
        "\n",
        "- Rossman Store Dataset.\n",
        "\n",
        "  - Number of observation - 1115 rows\n",
        "  - Number of columns - 10 columns\n",
        "  - Number of Missing Values - More than 30% missing in CompetitionOpenSinceMonth and CompetitonOpenSinceYear. More than 50% values missing in Promo2SinceWeek, Promo2SinceMonth and PromoInterval.\n",
        "  - Number of Duplicate Values - 0"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "\n",
        "# Rossman sales data columns\n",
        "df_sales.columns.to_list()"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rossman stores data columns\n",
        "df_stores.columns.to_list()"
      ],
      "metadata": {
        "id": "i4pci8ieozUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "# For Rossman Sales Data\n",
        "df_sales.describe(include = 'all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Rossman Store data\n",
        "df_stores.describe(include = 'all')"
      ],
      "metadata": {
        "id": "NCP1CB8spMpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the fields are self-explanatory. The following are descriptions for those that aren't.\n",
        "\n",
        "- Id - an Id that represents a (Store, Date) duple within the set\n",
        "\n",
        "- Store - a unique Id for each store\n",
        "\n",
        "- Sales - the turnover for any given day (Dependent Variable)\n",
        "\n",
        "- Customers - the number of customers on a given day\n",
        "\n",
        "- Open - an indicator for whether the store was open: 0 = closed, 1 = open\n",
        "\n",
        "- StateHoliday - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
        "\n",
        "- SchoolHoliday - indicates if the (Store, Date) was affected by the closure of public schools\n",
        "\n",
        "- StoreType - differentiates between 4 different store models: a, b, c, d\n",
        "\n",
        "- Assortment - describes an assortment level: a = basic, b = extra, c = extended. An assortment strategy in retailing involves the number and type of products that stores display for purchase by consumers.\n",
        "\n",
        "- CompetitionDistance - distance in meters to the nearest competitor store\n",
        "\n",
        "- CompetitionOpenSince[Month/Year] - gives the approximate year and month of the time the nearest competitor was opened\n",
        "\n",
        "- Promo - indicates whether a store is running a promo on that day\n",
        "\n",
        "- Promo2 - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n",
        "\n",
        "- Promo2Since[Year/Week] - describes the year and calendar week when the store started participating in Promo2\n",
        "\n",
        "- PromoInterval - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "\n",
        "# Unique Values for Rossman sales Data\n",
        "categorical_features_sales = ['DayOfWeek','Open','Promo','StateHoliday','SchoolHoliday']\n",
        "\n",
        "for col in categorical_features_sales:\n",
        "  print(f\"Unique values for {col} are : \",df_sales[col].unique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique Values for Rossman store data\n",
        "categorical_features_store = ['StoreType','Assortment','CompetitionOpenSinceMonth','CompetitionOpenSinceYear','Promo2','Promo2SinceYear','PromoInterval']\n",
        "\n",
        "for col in categorical_features_store:\n",
        "  print(f\"Unique values for {col} are : \",df_stores[col].unique())"
      ],
      "metadata": {
        "id": "sTfUs-lYrhMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Wrangling for Rossman Sales Dataset."
      ],
      "metadata": {
        "id": "hIO42QTOshor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Creating a copy of dataset\n",
        "dfs = df_sales.copy() "
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Converting the date column to Datetime datatype.\n",
        "dfs['Date'] = dfs['Date'].astype('datetime64[ns]')"
      ],
      "metadata": {
        "id": "ZS9kM-IEtPWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting Week, Month, Year from date for timewise analysis.\n",
        "# Creating new columns for WeekOfYear, Month and Year.\n",
        "\n",
        "# Extracting Year for yearly analysis\n",
        "dfs['Year'] = dfs['Date'].dt.year\n",
        "\n",
        "# Extracting Month from date for monthwise analysis\n",
        "dfs['Month'] = dfs['Date'].dt.month\n",
        "\n",
        "# Extracting WeekOfYear for weekwise analysis\n",
        "dfs['WeekOfYear'] = dfs['Date'].dt.weekofyear"
      ],
      "metadata": {
        "id": "WUuRSH-Utf8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the values of StateHoliday column into same datatype for ease of the analysis.\n",
        "dfs['StateHoliday'].replace({0 : '0'}, inplace = True)"
      ],
      "metadata": {
        "id": "Kt3HYy97um0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EDA(Exploratory Data Analysis) of Rossman Sales Dataset.**"
      ],
      "metadata": {
        "id": "qzFBa0a9vJ35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Univariate Analysis"
      ],
      "metadata": {
        "id": "6dZKq4gksL1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfs['Open'].value_counts()"
      ],
      "metadata": {
        "id": "i5qO_Ox5rQm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs['Promo'].value_counts()"
      ],
      "metadata": {
        "id": "h3Y2UwC9rZDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs['StateHoliday'].value_counts()"
      ],
      "metadata": {
        "id": "ByQugJztrmVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs['SchoolHoliday'].value_counts()"
      ],
      "metadata": {
        "id": "_dNwqS6Zr6AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bivariate Analysis"
      ],
      "metadata": {
        "id": "w1frLEzNsZ7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_daily_sales = dfs.groupby(['DayOfWeek'])['Sales'].mean().sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "CkO93xFtscSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_daily_sales"
      ],
      "metadata": {
        "id": "pvqOSNqesn8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Highest Average Sales are done on the 1st day of the week i.e. Monday.\n",
        "- Lowest average sales are made on 7th day of the week i.e. Sunday, indicating that most of the stores are closed on sunday."
      ],
      "metadata": {
        "id": "Be-V0V6Osr4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_monthly_sales = dfs.groupby(['Month'])['Sales'].mean().sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "9bb4YFt1spmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_monthly_sales"
      ],
      "metadata": {
        "id": "FP-bi26GtOQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Highest Monthly sales are made in the month of December, July and November."
      ],
      "metadata": {
        "id": "z7Axg-HdtRX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_sales_promo = dfs.groupby(['Promo'])['Sales'].mean().sort_values(ascending = False)\n",
        "avg_sales_promo"
      ],
      "metadata": {
        "id": "qUFQivultdh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Average Sales are more for the stores participating in the Promo.\n",
        "- Average Sales increase by almost 44% for stores participating in Promo."
      ],
      "metadata": {
        "id": "pU-j6zfLtkQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_sales_school_holiday = dfs.groupby(['SchoolHoliday'])['Sales'].mean().sort_values(ascending = False)\n",
        "avg_sales_school_holiday"
      ],
      "metadata": {
        "id": "ExPkhN4JuGiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Average Sales are high on school holidays, Indicating most of the stores are not closed on school holidays."
      ],
      "metadata": {
        "id": "KZWEbtQHuPT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_sales_State_holiday = dfs.groupby(['StateHoliday'])['Sales'].mean()\n",
        "avg_sales_State_holiday"
      ],
      "metadata": {
        "id": "IdBk5cg0uda-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Average Sales are very low on state holiday, indicating most of the stores are closed on state holidays."
      ],
      "metadata": {
        "id": "Rf7QINeDuoYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bivariate Analysis with 'Open' column."
      ],
      "metadata": {
        "id": "e4dp5JxCvLdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "store_open_school_holiday = dfs.groupby(['Open'])['SchoolHoliday'].value_counts()\n",
        "store_open_school_holiday"
      ],
      "metadata": {
        "id": "CVxk1Y69u9ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most of the stores stay open on School Holidays, Therefore more average sales."
      ],
      "metadata": {
        "id": "eSORj9O_vdM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "store_open_state_holiday = dfs.groupby(['Open'])['StateHoliday'].value_counts()\n",
        "store_open_state_holiday"
      ],
      "metadata": {
        "id": "VlWT_hftvw7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most of the stores stay closed on state holiday, therefore low average sales."
      ],
      "metadata": {
        "id": "UbteoTrZv5pQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "store_open_day_of_week = dfs.groupby(['Open'])['DayOfWeek'].value_counts()\n",
        "store_open_day_of_week"
      ],
      "metadata": {
        "id": "Gl0D_0WUwA7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most of the stores are closed on sunday, therefore low average sales on 7th day of week."
      ],
      "metadata": {
        "id": "fN4iyghnwJYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EDA(Explporatory Data Analysis) of Rossman Store Dataset.**"
      ],
      "metadata": {
        "id": "lcNs4Cv5xVtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a copy of the stores dataset for analysis\n",
        "dfst = df_stores.copy()"
      ],
      "metadata": {
        "id": "RApB8KUrxqmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Univariate Analysis"
      ],
      "metadata": {
        "id": "oD395cpMxgAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfst['StoreType'].value_counts()"
      ],
      "metadata": {
        "id": "fGpFd9O1wWzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There are more number of stores with type 'a'.\n",
        "- There are very less stores with type 'b'."
      ],
      "metadata": {
        "id": "omzdVxbjyHni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfst['Assortment'].value_counts()"
      ],
      "metadata": {
        "id": "Bb5MgP9lyVbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most of the stores work with assortment level 'a' and 'c'.\n",
        "- There are very less stores working with assortment level 'b'."
      ],
      "metadata": {
        "id": "AkqbzjnOyiNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Bivariate Analysis"
      ],
      "metadata": {
        "id": "LxNoJgDTzdgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Storetype_with_assortment = dfst.groupby(['StoreType'])['Assortment'].value_counts()"
      ],
      "metadata": {
        "id": "OQo60Wgfzjpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Storetype_with_assortment"
      ],
      "metadata": {
        "id": "jgSfx0uFz3rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- All the stores except for type 'b' work with 2 assortment levels 'a' and 'c'.\n",
        "- Store type 'b' work with all the 3 assortment level 'a','b' and 'c'."
      ],
      "metadata": {
        "id": "z4xBcZcSz8pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "storetype_with_promo2 = dfst.groupby(['StoreType'])['Promo2'].value_counts()"
      ],
      "metadata": {
        "id": "LjUpeRML0Qxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "storetype_with_promo2"
      ],
      "metadata": {
        "id": "m95wkWIu0rCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The number of stores running promo2 are slightly less than the ones running the promo2."
      ],
      "metadata": {
        "id": "qx2yNmVo00bW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging both the datasets.\n",
        "dataset = dfs.merge(right=dfst, on=\"Store\", how=\"left\")"
      ],
      "metadata": {
        "id": "qk52Z44i1VhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EDA(Exploratory Data Analysis) of the merged Dataset**"
      ],
      "metadata": {
        "id": "XuTlvi8S1-Rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a copy of the merged dataset.\n",
        "df1 = dataset.copy()"
      ],
      "metadata": {
        "id": "8Ar9wopF2HMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "id": "iTHEESdW2Run"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Multivariate Analysis"
      ],
      "metadata": {
        "id": "b-pLodG721sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "Z_dz1pNoxpfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = ['Open','Promo','StateHoliday','SchoolHoliday','StoreType','Assortment','Promo2']\n",
        "Numerical_features = ['Customers','CompetitionDistance']"
      ],
      "metadata": {
        "id": "rds8Z3q-xNVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store type with highest average sales and average customers.\n",
        "storetype_average_sales_customer = df1.groupby(['StoreType'])['Sales','Customers'].mean().sort_values(by = ['Sales','Customers'], ascending = [False,False])"
      ],
      "metadata": {
        "id": "cR-3QILz28Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "storetype_average_sales_customer"
      ],
      "metadata": {
        "id": "poveMmT14LWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Store type 'b' has highest average sales and customers."
      ],
      "metadata": {
        "id": "1S4lpip34OMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store type with highest sales and customer.\n",
        "storetype_sales_customer = df1.groupby(['StoreType'])['Sales','Customers'].sum().sort_values(by = ['Sales','Customers'], ascending = [False,False])\n",
        "storetype_sales_customer"
      ],
      "metadata": {
        "id": "fMOOhHHq4Wpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Store with highest total sales and customer is of type 'a'."
      ],
      "metadata": {
        "id": "Uv1B6Kt15Uei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets see what type of stores were open on what day of the week.\n",
        "storetype_open_dayofweek = df1.groupby(['DayOfWeek','Open'])['StoreType'].value_counts()\n",
        "storetype_open_dayofweek"
      ],
      "metadata": {
        "id": "R9NJ8E8W5c7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- All the stores of type 'c' stay closed on 7th day of the week(Sunday)."
      ],
      "metadata": {
        "id": "y8lT6iJj7urt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?\n"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manipulation**\n",
        "\n",
        "- Converted Date column to Datetime datatype.\n",
        "- Created columns WeekOfYear, Month, Year from date for timewise analysis.\n",
        "- Converted all the data from stateholiday to same datatype.\n",
        "\n",
        "**Insights**\n",
        "\n",
        "- Highest Average Sales are done on the 1st day of the week i.e. Monday.\n",
        "Lowest average sales are made on 7th day of the week i.e. Sunday, indicating that most of the stores are closed on sunday.\n",
        "\n",
        "- Highest Monthly sales are made in the month of December, July and November.\n",
        "\n",
        "- Average Sales increase by almost 44% for stores participating in Promo.\n",
        "\n",
        "- Average Sales are high on school holidays, Indicating most of the stores are not closed on school holidays.\n",
        "\n",
        "- Most of the stores stay closed on state holiday, therefore low average sales.\n",
        "\n",
        "- There are more number of stores with type 'a'.\n",
        "There are very less stores with type 'b'.\n",
        "\n",
        "- Most of the stores work with assortment level 'a' and 'c'.\n",
        "There are very less stores working with assortment level 'b'.\n",
        "\n",
        "- All the stores except for type 'b' work with 2 assortment levels 'a' and 'c'.\n",
        "Store type 'b' work with all the 3 assortment level 'a','b' and 'c'.\n",
        "\n",
        "- The number of stores running promo2 are slightly less than the ones running the promo2.\n",
        "\n",
        "- Store type 'b' has highest average sales and customers.\n",
        "\n",
        "- Store with highest total sales and customer is of type 'a'.\n",
        "\n",
        "- All the stores of type 'c' stay closed on 7th day of the week(Sunday)."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Value Counts of values in categorical variables\n",
        "for col in categorical_features:\n",
        "    counts = df1[col].value_counts().sort_index()\n",
        "    fig = plt.figure(figsize=(9, 6))\n",
        "    ax = fig.gca()\n",
        "    counts.plot.bar(ax = ax, color='steelblue')\n",
        "    ax.set_title(col + ' counts')\n",
        "    ax.set_xlabel(col) \n",
        "    ax.set_ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Countplot works best to show the value counts of all the categorical variables in dataset."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Stores participating in Promo are comparatively less than the ones not participating.\n",
        "\n",
        "- Store with type 'a' are more in number in compare to other types of stores.\n",
        "\n",
        "- Assortment level 'b' is used very less. "
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a very basic understanding of the value counts of the variable. We need to do some more analysis to find out the how do these variables Impact business."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Average Daily sales\n",
        "sns.barplot(x = df1['DayOfWeek'], y = df1['Sales'] ) "
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Barplot's default estimation function is mean, so here we use barplot to visualize average daily sales. "
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Highest average sales are seen on 1st day of the week(Monday).\n",
        "- Lowest average sales are seen on 7th day of the week(Sunday)."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph tells us the store can expect higher sales on monday, and the average sales are slighly lower on 4th and 6th day of the week and extremely low on sundays. So any other activities like service and mantainace of the store should be schedule in a way they could be carried out on the days when average sales are slightly low."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Average Monthly Sales.\n",
        "sns.barplot(x = df1['Month'], y = df1['Sales'] )"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.lineplot(x=\"Month\" ,y = \"Sales\" , data=df1) "
      ],
      "metadata": {
        "id": "eJBeFs8qlNCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Barplot's default estimation function is mean, so here we use barplot to visualize average Monthly sales.\n",
        " We have used linplot to show the monthly sales trend."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Highest average sales are seen in month of December, July and November."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sales volume are high in month of December, July and November, stores should plan their marketing campaign and promo accordingly to take full advantage of this."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Monthly Sales over the years \n",
        "sales_2013 = df1[df1['Year']== 2013]\n",
        "sales_2014 = df1[df1['Year']==2014]\n",
        "sales_2015 = df1[df1['Year']== 2015]\n",
        "\n",
        "# Monthly sales for respective years.\n",
        "sales_month_2013 = sales_2013.groupby('Month')['Sales'].sum().reset_index()\n",
        "sales_month_2014 = sales_2014.groupby('Month')['Sales'].sum().reset_index()\n",
        "sales_month_2015 = sales_2015.groupby('Month')['Sales'].sum().reset_index()\n",
        "\n",
        "# Plotting graph for monthly sale over years.\n",
        "plt.plot(sales_month_2013.loc[:,'Sales'],label='2013',color='blue')\n",
        "plt.plot(sales_month_2014.loc[:,'Sales'],label='2014',color='red')\n",
        "plt.plot(sales_month_2015.loc[:,'Sales'],label='2015',color='green')\n",
        "plt.title('Monthly Sales Over The Years')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used lineplot to show yearly sales trend."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The trend for 2013 and 2015 is almost similar. However there is sudden drop in month of July, August and September for year 2014."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sudden drop in the month of July, August and September for 2014 is because of the stores were closed for refurbishment which also mentioned in the problem statement."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Relationship between store type and assortment type and sales\n",
        "sns.barplot(x=df1[\"StoreType\"],y=df1['Sales'],hue=df1[\"Assortment\"])"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot default estimation is mean so we have used this graph to show the average sale for different store type with different assortment level they use."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The above bar plot shows that the store types 'a', 'c' and 'd' have only assortment level 'a' and 'c'. On the other hand the store type 'b' has all the three kinds of assortment strategies, a reason why average sales were high for store type 'b' stores."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store type 'b' with highest average sales and per store revenue generation looks healthy and a reason for that would be all three kinds of assortment strategies involved."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Plotting for storetype and sales\n",
        "df1.groupby(\"StoreType\")[\"Sales\"].sum().plot.pie(title='Store Type and Sales', legend=True, autopct='%1.1f%%', shadow=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot is used for average sales, I have used pie plot to show total sales with respect to the store types."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Earlier it was seen that store type 'b' has highest average sales, however it is clear for the above graph that the highest total sales is seen for store type 'a'."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of stores with type 'a' are more, probably the reason for more total sales are seen for store type 'a'. However store type 'b' seems more healthy in terms of average sales and per store revenue generation. Store type 'b' uses all three levels of assortment, this is a reason for the better performance in compare to the other stores."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Storetype vs customer\n",
        "df1.groupby(\"StoreType\")[\"Customers\"].sum().plot.pie(title='Store type and Customer', legend=True, autopct='%1.1f%%', shadow=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot is used for average sales, I have used pie plot to show total customer with respect to the store types."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Store Type 'a' has highest number of customer visit."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As discussed earlier Store type 'a' has more customer visits because there are more number of stores with type 'a'. However store type 'b' has better customer to sales ratio, because store type 'b' uses all three levels of assortment."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Percentage share of store type\n",
        "df1[\"StoreType\"].value_counts().plot.pie(title='Share of Store Types', legend=True, autopct='%1.1f%%', shadow=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pieplot is used to show the total number of all storetype present."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Store type 'a' is more in compare to other store type.\n",
        "- store type 'b' are least in number."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store type 'b' are better performing with good stratergy. Either store type 'b' should increase or the same stratergy should also be applied for other store type."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Lets explore the relationship between promo and sales\n",
        "sns.barplot(x=df1['Promo'],y=df1['Sales'],hue=df1['Promo2'])"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used barplot as it works perfectly for showing the relation between promo, promo2 and average sales."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sales almost doubles with promo on."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As it was seen earlier that the number of stores participating in promo are less than the ones that do not participate. It is very evident from the above the graph the sales increases by almost 44% with promo on."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Relation between sales and customer\n",
        "sns.scatterplot(x=df1['Customers'], y=df1['Sales'])"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used scatter plot here to show the strength of relationship between customers and sales."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen from the graph that more number of customers tends to more number of sales."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is evident from the above the graph that there is a direct positive correlation between customer and sales. Stores should plan stratergies to increase the customer walkins."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# Relation between CompetitionOpenSinceMonths and sales\n",
        "sns.barplot(x=df1['CompetitionOpenSinceMonth'], y=df1['Sales'])"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the impact of CompetitonOpenSince Month on sales."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no major impact on sales."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no major impact on sales."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Relationship between CompetitionDistance and sales.\n",
        "sns.scatterplot(x=df1['CompetitionDistance'], y=df1['Sales'])"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used scatter plot here to show the strength of relationship between customers and sales."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stores densely located near each other see more sales."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stores densely located near each other see more sales."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# Ploting the countplot to see the number of stores open on what day of the week.\n",
        "sns.countplot(x=df1[\"Open\"], hue=df1[\"DayOfWeek\"])"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyse the relationship between Open stores and day of week "
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it can be seen from the graph that very mostly stores are closed on sunday.\n",
        "It can also be seen some stores were closed on weekdays as well probably because of refurbishment or holidays."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it can be seen from the graph that very mostly stores are closed on sunday.\n",
        "It can also be seen some stores were closed on weekdays as well probably because of refurbishment or holidays."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(14,10))\n",
        "sns.heatmap(df1.corr(), cmap=\"coolwarm\", annot=True)"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the relationship between all the variables of Dataset."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Day of the week has a negative correlation indicating low sales as the weekends, and promo, customers and open has positive correlation.\n",
        "\n",
        "- State Holiday has a negative correlation suggesting that stores are mostly closed on state holidays indicating low sales.\n",
        "\n",
        "- CompetitionDistance showing negative correlation suggests that as the distance increases sales reduce, which was also observed through the scatterplot earlier.\n",
        "\n",
        "- There's multicollinearity involved in the dataset as well. The features telling the same story like Promo2, Promo2 since week and year are showing multicollinearity.\n",
        "\n",
        "- The correlation matrix is agreeing with all the observations done earlier while exploring through barplots and scatterplots."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Average Sales on 7th day of the week is less than otherdays.\n",
        "  - From EDA we found that average sales are very low on 7th day of the week which indicate that most of the stores are closed on sunday. So we will perform 2 sample t-test to find weather their is a decrease in average sales on 7th day of the week in compare to the otherdays. (Left Tailed Test)\n",
        "\n",
        "2. Store Type and Assortment level have some impact on sales.(Two Tailed Test)\n",
        "  \n",
        "  - From EDA we found that store type 'b' had highest average sales and per store revenue as it works with all three level of assortment level. We'll use two-way ANOVA test to find out what impact store type and assortment level have on sales.\n",
        "\n",
        "3. Promotion has some impact on sales.\n",
        "\n",
        "  - From EDA we found out that with promotion average sales increase by almost 44%. We'll use 2 sample t-test to check the impact of promotion on sales. "
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Null Hypothesis(H_O) : mean of other_days - mean of seventh_day = 0\n",
        "\n",
        "    (There is no difference in the mean of sales on other days of week and seventh day of the week.)\n",
        "\n",
        "- Alternate Hypothesis(H_A) : mean of other_days > mean of seventh_day\n",
        "\n",
        "    (Average Sales on other days of the week is greater than seventh day of the week.)\n",
        "\n",
        "- Significance level : alpha = 0.05"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating copy of dataframe for hypothesis testing.\n",
        "t_test_df = dataset.copy()"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seperating two groups for two-sample t-test\n",
        "other_days = t_test_df[t_test_df.DayOfWeek != 7]\n",
        "seventh_day = t_test_df[t_test_df.DayOfWeek == 7]\n",
        "\n",
        "# Creating 2 groups for two-sample t-test\n",
        "group_1 = np.array(other_days['Sales'])\n",
        "group_2 = np.array(seventh_day['Sales'])"
      ],
      "metadata": {
        "id": "nI_CHK64vZ21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "stats.ttest_ind(a=group_1, b=group_2, equal_var = False, alternative = 'greater')"
      ],
      "metadata": {
        "id": "Wq4NtSSjwALz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Test result for Business Significance : At the 0.05 level of significance, we do have strong evidence to conclude that other days of the week results in higher mean sales compared to seventh day of the week."
      ],
      "metadata": {
        "id": "P6xbBBx9w4JW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We Have perform a Two Sample t test for independent samples considering that the populations donot have equal variance.(Right tailed Hypothesis Testing)"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From EDA we found that the average sale for other days of the week is more than seventh day of the week. So we performed a Two Sample t test to find wether the increase in Avg. sales is significant or not.\n",
        "\n",
        "- Considering that the populations do not have equal variance and is unknown to us.\n",
        "\n",
        "- Right tailed Hypothesis Testing - As the consideration was the Increase is significant or not."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Null Hypothesis(H_O): There is no difference in average sales\n",
        "for any store type.\n",
        "\n",
        "  - Alternate Hypothesis(H_A) : There is a difference in average sales for any store type.\n",
        "\n",
        "2. Null Hypothesis(H_O): There is no difference in average sales with any assortment level.\n",
        "\n",
        "  - Alternate Hypothesis(H_A) : There is a difference in average sales with any assortment level.\n",
        "\n",
        "3. Null Hypothesis(H_O): The effect of Store type on average sales does not depend on the effect of the assortment level (a.k.a. no interaction effect).\n",
        "\n",
        "  - Alternate Hypothesis(H_A) : There is an interaction effect between assortment level and store type on average sales."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Performing two-way ANOVA\n",
        "anova_df = dataset.copy()\n",
        "anova_df.rename(columns={'Sales':'Sales'},inplace=True)\n",
        "model = ols('Sales ~ C(StoreType) + C(Assortment) +C(StoreType):C(Assortment)',data=anova_df).fit()\n",
        "result = sm.stats.anova_lm(model, type=2)\n",
        "  \n",
        "# Print the result\n",
        "print(result)\n",
        "     "
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Two-ANOVA test is performed to see how store type and assortment level impact average sales."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A two-way ANOVA was performed to analyze the effect of store type and assortment level on average sales.\n",
        "\n",
        "- The results showed that there is a statistically significant interaction effect between StoreType and Assortment (p = 0.000, which is less than the significance level of 0.05).This means that we can reject the null hypothesis of no interaction effect between StoreType and Assortment rented bike count.\n",
        "\n",
        "- There is a statistically significant effect of StoreType on Sales (p = 0.000, which is less than the significance level of 0.05).This means that we can reject the null hypothesis of no difference in mean values of the dependent variable among StoreType.\n",
        "\n",
        "- There is a statistically significant effect of Assortment on Sales (p = 0.00, which is less than the significance level of 0.05). This means that we can reject the null hypothesis of no difference in mean values of the dependent variable among Assortment."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Null Hypothesis(H_O) : mean without Promo - mean with Promo = 0\n",
        "\n",
        "    (There is no difference in the mean of sales with Promo and without Promo.)\n",
        "\n",
        "- Alternate Hypothesis(H_A) : mean without Promo < mean with Promo\n",
        "\n",
        "    (Average Sales for stores without Promo is less than the stores with Promo.)\n",
        "\n",
        "- Significance level : alpha = 0.05"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_test_promotion = dataset.copy()"
      ],
      "metadata": {
        "id": "RfXcCU0NyHEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seperating two groups for two-sample t-test\n",
        "no_promo = t_test_promotion[t_test_promotion.Promo == 0]\n",
        "with_promo = t_test_promotion[t_test_promotion.Promo == 1]\n",
        "\n",
        "# Creating two groups for two-sample t-test\n",
        "group_pr_1 = np.array(no_promo['Sales'])\n",
        "group_pr_2 = np.array(with_promo['Sales'])"
      ],
      "metadata": {
        "id": "p7RKziID0Qsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "stats.ttest_ind(a=group_pr_1, b=group_pr_2, equal_var = False, alternative = 'less')"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Test result for Business Significance : At the 0.05 level of significance, we do have strong evidence to conclude that average sales increase when stores participate in Promo."
      ],
      "metadata": {
        "id": "waexKNU91ffj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We Have perform a Two Sample t test for independent samples considering that the populations donot have equal variance.(left tailed Hypothesis Testing)"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From EDA we found out that with promotion average sales increase by almost 44%. so we performed 2 sample t-test to check the impact of promotion on sales.\n",
        "\n",
        "- Considering that the populations do not have equal variance and is unknown to us.\n",
        "\n",
        "- Left tailed Hypothesis Testing - As the consideration was the decrease is significant or not."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df1.info()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More than 50% of the values in Promo2SinceWeek, Promo2SinceYear, PromoInterval are missing so Dropping the columns\n",
        "df1.drop(['Promo2SinceWeek','Promo2SinceYear','PromoInterval'], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "52_UB33JQVqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the distribution for CompetitionDistance to impute the missing value either with mean or median.\n",
        "sns.displot(x=df_stores['CompetitionDistance'], kde = True)"
      ],
      "metadata": {
        "id": "-POLnbKhRGnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of CompetitionDistance is right skewed.\n",
        "# Imputing the missing value with median\n",
        "df1['CompetitionDistance'].fillna(df1['CompetitionDistance'].median(), inplace = True)"
      ],
      "metadata": {
        "id": "4xpPQtS1RbQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputing the missing values of CompetitionOpenSinceMonth and CompetitionOpenSinceYear with mode.\n",
        "df1['CompetitionOpenSinceMonth'].fillna(df1['CompetitionOpenSinceMonth'].mode()[0], inplace = True)\n",
        "df1['CompetitionOpenSinceYear'].fillna(df1['CompetitionOpenSinceYear'].mode()[0], inplace = True)"
      ],
      "metadata": {
        "id": "SUKAD_d9RsS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.isnull().sum()"
      ],
      "metadata": {
        "id": "m34YREnTR-23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- PromoSinceWeek, PromoSinceYear and PromoInterval had more than 50% values missing so we dropped the columns.\n",
        "\n",
        "- CompetitionDistance has a right skewed distribution so imputing the missing value with median.\n",
        "\n",
        "- Imputing the missing value of CompetitionSinceMonth and CompetitionSinceYear with mode."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "#code to seperate outliers\n",
        "mean_sales = np.mean(df1['Sales']) #mean\n",
        "sd_sales = np.std(df1['Sales'])   #standard deviation\n",
        "#More than 3 standard deviation is an outlier\n",
        "threshold = 3\n",
        "#code to identify them\n",
        "outliers = []\n",
        "for value in df1['Sales']:\n",
        "    z_score = (value-mean_sales)/sd_sales\n",
        "    if z_score > threshold:\n",
        "        outliers.append(value)\n",
        "#total no of outliers        \n",
        "print(f'Total number of Outliers present in the Sales column are {len(outliers)}.')\n",
        "#plotting the outlier distribution\n",
        "sns.distplot(x=outliers).set(title='Outliers Distribution')"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#percentage of sales greater than 10.2\n",
        "sales_outliers = df1.loc[df1['Sales']> 28000]\n",
        "percentage_of_outliers = (len(sales_outliers)/len(df1))*100\n",
        "#print\n",
        "print(f'The percentage of observations of sales greater than 28000 are {percentage_of_outliers}')"
      ],
      "metadata": {
        "id": "O9zK_nchTYTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the reason behind this behaviour.\n",
        "sales_outliers"
      ],
      "metadata": {
        "id": "XUxCoMsMTlpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It can be seen that the outliers are showing this behavior for the stores with promotion on and store type 'b'. It would not be wise to treat them because the reasons behind this behavior seems fair."
      ],
      "metadata": {
        "id": "6fw3FBLQT_RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lets see which stores were open on Sunday in the outliers dataframe\n",
        "#store 262\n",
        "sales_outliers.loc[sales_outliers['DayOfWeek']==7]"
      ],
      "metadata": {
        "id": "E5bCdNUIWF8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's explore store type and Day Of week\n",
        "sns.barplot(x=df1['DayOfWeek'],y=df1[\"Sales\"],hue=df1['StoreType'])"
      ],
      "metadata": {
        "id": "Qv6UD4g0WLb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This suggests that store type b had high sales almost all week. No store of type C was open on Sunday.\n",
        "- Being open 24*7 along with all kinds of assortments available is probably the reason why it had higher average sales than any other store type."
      ],
      "metadata": {
        "id": "S-qcuZTPWqOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It can be well established that the outliers are showing this behaviour for the stores with promotion on and store type 'b'. It would not be wise to treat them because the reasons behind this behaviour seems fair and important from the business point of view.\n",
        "- The primary reasons for the behaviour are promotion and store type 'b'.\n",
        "- If the outliers are a valid occurrence it would be wise not to treat them by deleting or manipulating them."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the values for stateholiday.\n",
        "df1['StateHoliday'].replace({'0' : 0, 'a' : 1, 'b' : 1, 'c' : 1}, inplace = True)"
      ],
      "metadata": {
        "id": "0Z5PEZMm3RdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For State Holiday data point with '0' were around 90% and other data point 'a', 'b', 'c' were around 4% each. So we converted the values, '0' to 0 and 'a','b','c' to 1.\n",
        "\n",
        "- We'll use One Hot Encoding for StoreType and Assortment as the data is nominal and not ordinal after spliting the data for model implemntation."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# Combining CompetitionOpenSinceMonth and CompetitionOpenSinceYear to Competition_open_total_months\n",
        "df1['Competition_open_total_months'] = (df1['Year'] - df1['CompetitionOpenSinceYear'])*12 + (df1['Month'] - df1['CompetitionOpenSinceMonth'])\n",
        "\n",
        "# For no negative values\n",
        "df1['Competition_open_total_months'] = df1['Competition_open_total_months'].apply(lambda x:0 if x < 0 else x)\n",
        "\n",
        "# Dropping CompetitionOpenSinceMonth and CompetitionOpenSinceYear\n",
        "df1.drop(['CompetitionOpenSinceMonth','CompetitionOpenSinceYear'], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Closed store would generate 0 sales so droping those values.\n",
        "df1 = df1[df1.Open != 0]\n",
        "df1.drop('Open',axis =1, inplace = True)"
      ],
      "metadata": {
        "id": "uFZigG7C-KVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# Dropping 'Store' and 'Date' as we do need them for prediction.\n",
        "df1.drop(['Store', 'Date'], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using VIF for checking multicolinearity.\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "MIT4tIT58WKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df1[[i for i in df1.describe().columns if i not in ['Sales']]])"
      ],
      "metadata": {
        "id": "AvfOYYxR91U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df1[[i for i in df1.describe().columns if i not in ['Sales','WeekOfYear']]])"
      ],
      "metadata": {
        "id": "fRIIlyKw-o6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df1[[i for i in df1.describe().columns if i not in ['Sales','WeekOfYear','Year']]])"
      ],
      "metadata": {
        "id": "jut_juVS-6wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.drop(['WeekOfYear','Year'], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "3I82eITv_NgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We have used VIF to check what all continuous features follow multicolinearity and select the feature accordingly.\n",
        "\n",
        "- We directly selected only the data points where the stores were open as closed store generate 0 sales.\n",
        "\n",
        "- We do not need Store and Date Column for prediction, so we dropped them directly."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Important Continuous features are - 'Customers', 'CompetetionDistance'.\n",
        "\n",
        "- Important Categorical Features are - 'StoreType','Assortment','DayOfWeek','Promo','StateHoliday','SchoolHoliday','Month','Promo2','Competition_open_total_months'."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Distplot to check the distribution of the output variable.\n",
        "sns.distplot(x=df1['Sales'])"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Target Variable is right skewed so we need to transform it to normal or near normal distribution.\n",
        "\n",
        "- We will apply squareroot transformation on target variable to transform it to near normal distribution."
      ],
      "metadata": {
        "id": "qdkOKY50B_zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(x=np.sqrt(df1['Sales']))"
      ],
      "metadata": {
        "id": "3xANrXu0CjBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "x = df1.drop('Sales', axis = 1)\n",
        "y = np.sqrt(df1['Sales'])"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "id": "bvnjD1KpypWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "c55C4eWCD4u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "_rN_YGurD8db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x, y, test_size = 0.25, random_state = 42)"
      ],
      "metadata": {
        "id": "9GdeUpPHD_Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "id": "dLQt5cVlEcAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "id": "SeMJMq2bEe5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "XOMaWyonEieG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "id": "rIb-QuEBEk8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best train test split ratio depends on the size and complexity of your dataset, the type of machine learning problem you are solving.\n",
        "\n",
        "For small datasets, we can use a 60:40 or 70:30 split for train and test sets. So, we have used 75:25 ratio for train test split."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Categorical Encoding"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OneHotEncoding on Categorical columns 'StoreType' and 'Assortment'\n",
        "transformer = ColumnTransformer(transformers = [('tnf',OneHotEncoder(drop = 'first', sparse = False, handle_unknown = 'ignore'),['StoreType','Assortment'])],remainder = 'passthrough')\n",
        "x_train_transformed = transformer.fit_transform(x_train)\n",
        "x_test_transformed = transformer.transform(x_test)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "x_train_scaled = scaler.fit_transform(x_train_transformed)\n",
        "x_test_scaled = scaler.transform(x_test_transformed)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A standard scaler is a data preprocessing technique that transforms the numerical features of a dataset to have a mean of zero and a standard deviation of one, which can improve the performance of linear regression, polynomial regrsiion and support vector machines.\n",
        "\n",
        "It scales the features to a common range, which can help compare different features and avoid dominance of some features over others due to their large magnitude"
      ],
      "metadata": {
        "id": "ThIUkrHaFmxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression (OLS)**"
      ],
      "metadata": {
        "id": "Q1c5SM0kJpru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# Linear Regression (OLS)\n",
        "lr =  LinearRegression()\n",
        "# Fit the Algorithm\n",
        "lr.fit(x_train_scaled,y_train)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_test_pred = lr.predict(x_test_scaled)\n",
        "y_train_pred = lr.predict(x_train_scaled)"
      ],
      "metadata": {
        "id": "wOH3F4iuG9iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_matric(y_test,y_test_pred,X_test_shape):\n",
        "\n",
        "  MSE = round(mean_squared_error(y_test,y_test_pred),2)\n",
        "  print(f\"MSE                : {MSE}\")\n",
        "\n",
        "  RMSE = round(np.sqrt(MSE),2)\n",
        "  print(f\"RMSE               : {RMSE}\")\n",
        "\n",
        "  r_squared = round(r2_score(y_test,y_test_pred),2)\n",
        "  print(f\"r2                 : {r_squared}\")\n",
        "\n",
        "  Adjusted_r_squared = round(1 - ((1 - r_squared) * (X_test_shape[0] - 1) / (X_test_shape[0] - X_test_shape[1] -1)),2)\n",
        "  print(f\"Adjusted r squared : {Adjusted_r_squared}\")\n",
        "\n",
        "  return (MSE,RMSE,r_squared,Adjusted_r_squared)"
      ],
      "metadata": {
        "id": "arpOh_pnOYE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.intercept_"
      ],
      "metadata": {
        "id": "5V5QVftcHQAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.coef_"
      ],
      "metadata": {
        "id": "LVr29-P1HTBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.score(x_train_scaled,y_train)"
      ],
      "metadata": {
        "id": "EeToRX6iINlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.score(x_test_scaled,y_test)"
      ],
      "metadata": {
        "id": "u8RIfG5BIR6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "mse,rmse,r2,adjusted_r2 =  eval_matric(y_test**2,y_test_pred**2,x_test_scaled.shape)\n",
        "\n",
        "# Creating A DataFrame to store Evaluation Metrics values for linear regression with and without regularization\n",
        "linear_eval_matric = pd.DataFrame(columns=['model_name','Regularization','RMSE(Avg_error_in_prediction)','Adjusted_r2'])\n",
        "# Appending the evaluation Metric values in DataFrame\n",
        "linear_eval_matric.loc[len(linear_eval_matric.index)] =  ['OLS','none',rmse,adjusted_r2]\n",
        "\n",
        "\n",
        "# Creating A DataFrame to store Evaluation Metrics values for Different regression with best results after Hyperparameter Tuning and Regularization.\n",
        "eval_matric_df = pd.DataFrame(columns=['model_name','RMSE(Avg_error_in_prediction)','Adjusted_r2'])\n",
        "# Appending the values in DataFrame\n",
        "eval_matric_df.loc[len(eval_matric_df.index)] =  ['OLS',rmse,adjusted_r2]"
      ],
      "metadata": {
        "id": "VypsxQhGSN21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_eval_matric"
      ],
      "metadata": {
        "id": "1jOtskjqS6df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Creating a Object for Lasso Regression\n",
        "lasso = Lasso()\n",
        "\n",
        "# Creating Dictionary of Hyperparameters\n",
        "parameters = {'alpha': [1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "\n",
        "# Implementing the the regression with GridSearch CV  \n",
        "lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_root_mean_squared_error', cv=5)\n",
        "# Fitting the best model\n",
        "lasso_regressor.fit(x_train_scaled, y_train)\n",
        "\n",
        "# Predicting the Test Values\n",
        "y_pred_test_lasso = lasso_regressor.predict(x_test_scaled)\n",
        "# Visualizing evaluation Metric Score chart\n",
        "mse,rmse,r2,adjusted_r2 = eval_matric((y_test)**2,(y_pred_test_lasso)**2,x_test_scaled.shape)\n",
        "\n",
        "# Appending the values in the DataFrame\n",
        "linear_eval_matric.loc[len(linear_eval_matric.index)] =  ['Lasso','L1',rmse,adjusted_r2]"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperprarameter tuning - Ridge Regression\n",
        "\n",
        "# Creating a Object for Ridge Regression\n",
        "ridge = Ridge()\n",
        "\n",
        "# Creating Dictionary of Hyperparameters\n",
        "parameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "\n",
        "# Implementing the the regression with GridSearch CV  \n",
        "ridge_regressor = GridSearchCV(ridge, parameters, scoring='neg_root_mean_squared_error', cv=3)\n",
        "# Fitting the best model\n",
        "ridge_regressor.fit(x_train_scaled,y_train)\n",
        "\n",
        "# Predicting the Test Values\n",
        "y_pred_test_ridge = ridge_regressor.predict(x_test_scaled)\n",
        "# Visualizing evaluation Metric Score chart\n",
        "mse,rmse,r2,adjusted_r2 = eval_matric((y_test)**2,(y_pred_test_ridge)**2,x_test_scaled.shape)\n",
        "\n",
        "# Appending the values in the DataFrame\n",
        "linear_eval_matric.loc[len(linear_eval_matric.index)] =  ['Ridge','L2',rmse,adjusted_r2]"
      ],
      "metadata": {
        "id": "ait68lGGsTfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperprarameter tuning - ElasticNet Regression\n",
        "\n",
        "# Creating a Object for ElasticNet Regression\n",
        "elastic = ElasticNet()\n",
        "\n",
        "# Creating Dictionary of Hyperparameters\n",
        "parameters = {'alpha': [1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100],'l1_ratio':np.linspace(0.1,1,10)}\n",
        "\n",
        "# Implementing the the regression with GridSearch CV  \n",
        "elastic_regressor = RandomizedSearchCV(elastic, parameters, scoring='neg_root_mean_squared_error',cv=4)\n",
        "# Fitting the best model\n",
        "elastic_regressor.fit(x_train_scaled, y_train)\n",
        "\n",
        "# Predicting the Test Values\n",
        "y_pred_test_en = elastic_regressor.predict(x_test_scaled)\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "mse,rmse,r2,adjusted_r2 = eval_matric((y_test)**2,(y_pred_test_en)**2,x_test_scaled.shape)\n",
        "\n",
        "# Appending the values in the DataFrame\n",
        "linear_eval_matric.loc[len(linear_eval_matric.index)] =  ['ElasticNet','L1 and L2',rmse,adjusted_r2]"
      ],
      "metadata": {
        "id": "MS_Hw1IwtcNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_regressor.best_params_"
      ],
      "metadata": {
        "id": "tPXLRTu7x5Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_regressor.best_params_"
      ],
      "metadata": {
        "id": "wMF5vaE_yB2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elastic_regressor.best_params_"
      ],
      "metadata": {
        "id": "k4fHnl5uyKcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have applied Lasso, Ridge and Elasticnet Regressor.\n",
        "\n",
        "lasso regression - Initially tried by applying L1 regularization and using GridSearchCV. Used Gridsearch CV as the Variables are less and also the variables are less so it wont be computaionally heavy. And comparing to Randomized Search CV we will get best results in first try.\n",
        "\n",
        "But the Best value of alpha has the same RMSE of 1483 i.e. the error in prediction.\n",
        "\n",
        "Ridge Regression - Initially tried by applying L2 regularization and using GridSearchCV.\n",
        "\n",
        "But the Best value of alpha has the same RMSE of 1483 i.e. the error in prediction.\n",
        "\n",
        "Finally tried ElasticNet Regularization and that is L1 and L2 combine and did a gridsearch CV to find the best Parameters. and improve the accuracy. But the results were the same."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "linear_eval_matric"
      ],
      "metadata": {
        "id": "jWd5-Z54uvO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There was no improvement. We will try Polynomial Regression next and see if we can get a better accuracy. "
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Polynomial Regression**"
      ],
      "metadata": {
        "id": "DRELVQsGF4GN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "\n",
        "# Creating Polynomial Features object with degree = 2\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "\n",
        "# transforms the existing features to higher degree features.\n",
        "X_train_poly = poly_features.fit_transform(x_train_scaled)\n",
        "X_test_poly = poly_features.transform(x_test_scaled)\n",
        "\n",
        "# fit the transformed features to Linear Regression\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# predicting on test data-set\n",
        "y_pred_test_poly = poly_model.predict(X_test_poly)\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "mse,rmse,r2,adjusted_r2 = eval_matric((y_test)**2,(y_pred_test_poly)**2,X_test_poly.shape)\n",
        "\n",
        "# Creating A DataFrame to store Evaluation Metrics values for Polynomial regression with and without regularization\n",
        "poly_eval_matric = pd.DataFrame(columns=['Reg_name','Regularization','RMSE(Avg_error_in_prediction)','Adjusted_r2'])\n",
        "poly_eval_matric.loc[len(poly_eval_matric.index)] =  ['Polynomial','none',rmse,adjusted_r2]\n",
        "\n",
        "# eval_matrix = pd.DataFrame(columns=['Reg_name','RMSE(Avg_error_in_prediction)','Adjusted_r2'])\n",
        "eval_matric_df.loc[len(eval_matric_df.index)] =  ['polynomial',rmse,adjusted_r2]"
      ],
      "metadata": {
        "id": "OVqX3h9UwcC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Creating a Object for ElasticNet Regression\n",
        "elastic_poly = ElasticNet(tol=0.1)\n",
        "\n",
        "# Creating Dictionary of Hyperparameters\n",
        "parameters = {'alpha': [1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100],'l1_ratio':np.linspace(0.1,1,10)}\n",
        "\n",
        "# Implementing the the regression with GridSearch CV  \n",
        "elastic_regressor_poly = RandomizedSearchCV(elastic_poly, parameters, scoring='neg_root_mean_squared_error',cv=4)\n",
        "\n",
        "# Fitting the best model\n",
        "elastic_regressor_poly.fit(X_train_poly, y_train)\n",
        "\n",
        "# Predicting the Test Values\n",
        "y_pred_test_ep = elastic_regressor_poly.predict(X_test_poly)\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "mse,rmse,r2,adjusted_r2 = eval_matric((y_test)**2,(y_pred_test_ep)**2,X_test_poly.shape)\n",
        "\n",
        "# Appending the values in the DataFrame\n",
        "poly_eval_matric.loc[len(poly_eval_matric.index)] =  ['Polynomial','Elasticnet l1 and l2',rmse,adjusted_r2]"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elastic_regressor_poly.best_params_"
      ],
      "metadata": {
        "id": "1uHouQ1rAf7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used Randomizes search cv as it will help reduce the computational time. "
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_eval_matric"
      ],
      "metadata": {
        "id": "JnvpWNUxCjhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial Regression with no regularization is performing slightly better than polynomial regression with l1 and l2 norm."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing the RMSE of both the model it can be seen that RMSE for polynomial regression has decreased. So the prediction made by polynomial regression will be more accurate compared to linear regression.\n",
        "\n",
        "Also the variance explained by polynomial model is high i.e. 87%, 10% higher than linear model."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree**"
      ],
      "metadata": {
        "id": "ezyOhVyXGOPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "dt_regressor = DecisionTreeRegressor()\n",
        "# Fit the Algorithm\n",
        "dt_regressor.fit(x_train_scaled, y_train)\n",
        "# Predict on the model\n",
        "y_test_pred_dt = dt_regressor.predict(x_test_scaled)\n",
        "y_train_pred_dt = dt_regressor.predict(x_train_scaled)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "mse,rmse,r2,adjusted_r2 = eval_matric((y_test)**2,(y_test_pred_dt)**2,x_test_scaled.shape)\n",
        "eval_matric_df.loc[len(eval_matric_df.index)] =  ['Decision Tree',rmse,adjusted_r2]"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_eval_matric = pd.DataFrame(columns=['Model_Name','Hyperparameter Tuning','RMSE(Avg_error_in_prediction)','Adjusted_r2'])\n",
        "dt_eval_matric.loc[len(dt_eval_matric.index)] =  ['Decision Tree','none',rmse,adjusted_r2]"
      ],
      "metadata": {
        "id": "903QT0vj1_Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Creating a Object for DecisionTreeRegressor\n",
        "dt_reg_gs = DecisionTreeRegressor()\n",
        "\n",
        "# Creating Dictionary of Hyperparameters\n",
        "parameters = {'criterion': ['squared_error'],'max_depth': [4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\n",
        "\n",
        "# Implementing the the regression with Randomized CV  \n",
        "dt_gs_cv = RandomizedSearchCV(dt_reg_gs,parameters,cv=5)\n",
        "\n",
        "# Fit the Algorithm\n",
        "dt_gs_cv.fit(x_train_scaled,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_test_dt_gs = dt_gs_cv.predict(x_test_scaled)\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "eval_matric((y_test)**2,(y_pred_test_dt_gs)**2,x_test_scaled.shape)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_eval_matric.loc[len(dt_eval_matric.index)] =  ['Decision Tree','RandomizedsearchCV',rmse,adjusted_r2]"
      ],
      "metadata": {
        "id": "KMdXDpKB2zvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_eval_matric"
      ],
      "metadata": {
        "id": "jbRFdaaufphd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_gs_cv.best_params_"
      ],
      "metadata": {
        "id": "flaB_H20drQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used max_depth as the parameter for Hyper parameter tuning. As the Decision Tree Regressor is a Greedy algorithm, I have used Randomized Search CV."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With max 20 RMSE decreased from 745.14 to 723.83. And the r2 score improved from 0.94 to 0.95."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4"
      ],
      "metadata": {
        "id": "OPgM6qwWMdEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**"
      ],
      "metadata": {
        "id": "agY6UgPBMdEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "UY5iabxaMdEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "rd_reg = RandomForestRegressor(n_estimators=100,min_samples_split=2, min_samples_leaf=1,max_depth=None,n_jobs=-1)\n",
        "\n",
        "# Fit the Algorithm\n",
        "rd_reg.fit(x_train_scaled, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_test_rd = rd_reg.predict(x_test_scaled)\n",
        "y_pred_train_rd = rd_reg.predict(x_train_scaled)"
      ],
      "metadata": {
        "id": "LpdSMlBoMdEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "mse,rmse,r2,adjusted_r2 = eval_matric((y_test)**2,(y_pred_test_rd)**2,x_test_scaled.shape)\n",
        "eval_matric_df.loc[len(eval_matric_df.index)] =  ['Random Forest',rmse,adjusted_r2]"
      ],
      "metadata": {
        "id": "uixLFGASgYEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_eval_matric = pd.DataFrame(columns=['Model_Name','Hyperparameter Tuning','RMSE(Avg_error_in_prediction)','Adjusted_r2'])\n",
        "rf_eval_matric.loc[len(rf_eval_matric.index)] =  ['Decision Tree','none',rmse,adjusted_r2]"
      ],
      "metadata": {
        "id": "AICQ0jSh3elL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_eval_matric"
      ],
      "metadata": {
        "id": "mHhs8fuof4kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "zxW1zMjLMdEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the best model\n",
        "#rd_regressor = RandomForestRegressor(n_jobs=-1)\n",
        "\n",
        "#params = {\n",
        "#          'n_estimators':[40,50,60,70,80,90],\n",
        "#          'min_samples_split':[2,3,6,8],\n",
        "#          'min_samples_leaf':[1,2,3,4],\n",
        "#          'max_depth':[None,5,15,30]\n",
        "#          }\n",
        "\n",
        "#rd_gs_cv = RandomizedSearchCV(estimator = rd_regressor, param_distributions = params, verbose=True, cv=10)\n",
        "\n",
        "#rd_gs_cv.fit(x_train_scaled, y_train)\n",
        "\n",
        "# Predicting the Test Values\n",
        "#y_pred_test_rd_gs = rd_gs_cv.predict(x_test_scaled)\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "#eval_matric((y_test)**2,(y_pred_test_rd_gs)**2,x_test_scaled.shape)"
      ],
      "metadata": {
        "id": "UREBByCSMdEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "uZMu8WigMdEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used Randomizes search cv as it will help reduce the computational time. "
      ],
      "metadata": {
        "id": "ybppV6m3MdEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "tSG8Qni2MdEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With parameters { n_estimators=80,min_samples_split=2,min_samples_leaf=1,max_depth=None }, the model is able to explain 99% variance."
      ],
      "metadata": {
        "id": "obcIWuWSMdEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "-CCHL3tWMdEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing all the models, random forest proves to be most accurate with 97% r2_score. Apply random forest will give more accurate prediction."
      ],
      "metadata": {
        "id": "xlLRnF2ZMdEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "SpU2WZvv16jL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different metrics may be more suitable for different purposes and audiences. For example, some metrics may focus on the accuracy of predictions, while others may focus on the variability explained by the model. The Metrics used in the Project are.\n",
        "\n",
        "- **RMSE Root Mean Squared Error :** It measures average error there is in the predictions. 536 is the average prediction error for a sale\n",
        "\n",
        "- **Adjusted r squared :** it measures the proportion of the variance in the response variable that is explained by the predictor variables, adjusted for the number of predictors. It is a modified version of r2 that takes into account the number of predictors and the sample size. It penalizes the model for adding irrelevant predictors.\n",
        "\n",
        "**Best Results :** Random Forest model has the Lowest values for RMSE i.e. 536 and Highest value for Adjusted r squared, i.e. 0.97 which means 97% variability can be explained using the model."
      ],
      "metadata": {
        "id": "LdO-DZa61-Le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_matric_df"
      ],
      "metadata": {
        "id": "3tPX7RQvnk8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameter_df = pd.DataFrame(columns = ['Model_Name', 'Best Parameter'])\n",
        "best_parameter_df.loc[len(best_parameter_df.index)] =  ['Lasso Linear Regression', lasso_regressor.best_params_]\n",
        "best_parameter_df.loc[len(best_parameter_df.index)] =  ['Ridge Linear Regression', ridge_regressor.best_params_]\n",
        "best_parameter_df.loc[len(best_parameter_df.index)] =  ['ElasticNet Linear Regression', elastic_regressor.best_params_]\n",
        "best_parameter_df.loc[len(best_parameter_df.index)] =  ['ElasticNet Polynomial Regression', elastic_regressor_poly.best_params_]\n",
        "best_parameter_df.loc[len(best_parameter_df.index)] =  ['Decision Tree', dt_gs_cv.best_params_]\n",
        "best_parameter_df.loc[len(best_parameter_df.index)] =  ['Random Forest', '{ n_estimators=80,min_samples_split=2,min_samples_leaf=1,max_depth=None }']"
      ],
      "metadata": {
        "id": "YLArbDfUG-yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "metadata": {
        "id": "TvvM7AW1K-Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(best_parameter_df)"
      ],
      "metadata": {
        "id": "yJ-S7ZMUJx0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest model has the Lowest values for RMSE i.e. 536 and Highest value for Adjusted r squared, i.e. 0.97 which means 97% variability can be explained using the model."
      ],
      "metadata": {
        "id": "0G9KZKgh3-B-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#shap.initjs()\n",
        "\n",
        "# Explainer Object for light Gradient Boost Regressor\n",
        "#explainer = shap.Explainer(rd_reg.predict, x_train_scaled)\n",
        "# Genrating the shapley values for the model.\n",
        "#shap_values = explainer(x_test_scaled)"
      ],
      "metadata": {
        "id": "TwHWGIHPhR6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Function for displaying Waterfall and condensed plot for a given data:\n",
        "#def sample_feature_importance(idx, type='condensed'):\n",
        "#    if type == 'condensed':\n",
        "#        return shap.plots.force(shap_values[idx], matplotlib=True)\n",
        "        \n",
        "#    elif type == 'waterfall':\n",
        "#        return shap.plots.waterfall(shap_values[idx])\n",
        "    \n",
        "#    else:\n",
        "#        return \"Return valid visual ('condensed', 'waterfall')\""
      ],
      "metadata": {
        "id": "H9HDdnHxP97l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the feature importance plot\n",
        "# shap_values = explainer(x_train_scaled)\n",
        "#shap.plots.bar(shap_values)"
      ],
      "metadata": {
        "id": "pzSxlU6DQGZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The idea behind SHAP feature importance is simple: Features with large absolute Shapley values are important.\n",
        "\n",
        "- Since we want the global importance, we average the absolute Shapley values per feature across the data\n",
        "\n",
        "- The feature importance plot is useful, but contains no information beyond the importances. For a more informative plot, we will next look at the summary plot.\n",
        "\n",
        "- The summary plot combines feature importance with feature effects. Each point on the summary plot is a Shapley value for a feature and an instance\n",
        "\n",
        "- We get a sense of the distribution of the Shapley values per feature. The features are ordered according to their importance."
      ],
      "metadata": {
        "id": "WY-vwE7CQZoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the Summary Plot for Shapley values\n",
        "#shap.summary_plot(shap_values, x_test_scaled)"
      ],
      "metadata": {
        "id": "hcIcb8akQTsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shap.plots.beeswarm(shap_values)"
      ],
      "metadata": {
        "id": "-ebxvmL8Qs3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The baseline for Shapley values is the average of all predictions. In the plot, each Shapley value is an arrow that pushes to increase (positive value) or decrease (negative value) the prediction. These forces balance each other out at the actual prediction of the data instance."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the condensed plot for and sample\n",
        "#sample_feature_importance(1,'condensed')"
      ],
      "metadata": {
        "id": "qiXUXSCCQ0BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the Waterfall plot for and sample\n",
        "#sample_feature_importance(1,'waterfall')"
      ],
      "metadata": {
        "id": "9VKNjMioQ4qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing Feature Importance for Random Forest"
      ],
      "metadata": {
        "id": "L7Ai8HHP2cbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_new = x.copy()"
      ],
      "metadata": {
        "id": "3S0VZLzFzd_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_new = pd.get_dummies(x_new, columns = ['StoreType','Assortment'], drop_first = True)"
      ],
      "metadata": {
        "id": "gZzuFTHhzAFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_df = pd.DataFrame(x_test_scaled, columns = x_new.columns)"
      ],
      "metadata": {
        "id": "XBz7Y3ZF0WF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualising Feature Importance for Random Forest\n",
        "feature_imp = pd.DataFrame({\"Variable\": columns_df.columns,\"Importance\": rd_reg.feature_importances_})\n",
        "feature_imp.sort_values(by=\"Importance\", ascending=False, inplace = True)\n",
        "sns.barplot(x=feature_imp['Importance'], y= feature_imp['Variable'])"
      ],
      "metadata": {
        "id": "0BXY3AZk1GkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_df = pd.DataFrame(y_test)"
      ],
      "metadata": {
        "id": "Qm54oUqaQuu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sales Predictions\n",
        "y_pred_test_rf = rd_reg.predict(x_test_scaled)\n",
        "sales_prediction_df = y_test_df.copy()\n",
        "sales_prediction_df['Pred_Sales'] = y_pred_test_rf"
      ],
      "metadata": {
        "id": "n-gh4OIpQBaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_prediction_df['Sales'] = sales_prediction_df['Sales']**2"
      ],
      "metadata": {
        "id": "QVrUhsiRSMLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_prediction_df['Pred_Sales'] = sales_prediction_df['Pred_Sales']**2"
      ],
      "metadata": {
        "id": "wxtZZc91S9gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_prediction_df.head()"
      ],
      "metadata": {
        "id": "YLtS1AMeS-62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- With Promotion there is an increase in average sales by almost 44%. Promotion also have a positive impact on customer. However there are comparatively less number of stores particiapting in Promotion. So more stores should be encouraged to participate in Promotions.\n",
        "\n",
        "- Store type 'b', in spite being less in number has highest average sales and per store revenue, as it works with all three assortment levels and is also open on sunday(7th day of the week). There should be more number of store with type 'b' or the same stratergies as store type 'b' should also be applied with other store type.\n",
        "\n",
        "- Talking about competition distance, store densly located near eachother experience more sales then the ones that are located further.\n",
        "\n",
        "- Random Forest model fits best on this data, with least average error i.e. 536, and variance explained by Random Forest is highest in compare to other models i.e. 97%. Deploying Random Forest can give more accurate sales prediction."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}